{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b872aa-c1e9-42b8-b966-2ca81a5c9315",
   "metadata": {},
   "source": [
    "# LFM Toy Model\n",
    "This notebook is used to fine-tune a DinoV3 toy model on Lunar data. \n",
    "\n",
    "# Model specifications\n",
    "The SAT-493M ViT-L/16 distilled DinoV3 encoder was used (trained on Satellite data). All encoder parameters were unfrozen for fine-tuning. See the [DinoV3 repo](https://github.com/facebookresearch/dinov3) for more info. \n",
    "\n",
    "## Input data specifications\n",
    "The vis data, (hosted at /explore/nobackup/projects/lfm/rawdata/Lunar/LowRes_MLDataset_v1_bilinear), was preprocessed by extracting the following bands and normalizing values to [0,1] range: [643, 566, 415]. Data was saved in (3, 300, 300) shape .npy files under the LFM project space (explore/nobackup/projects/lfm/vis_chips). \n",
    "\n",
    "## Label specifications\n",
    "Labels were processed from the annotations JSON file. Annotations were sorted by corresponding filename, then all labels for a given filename were saved single composite (300, 300) shape .npy images under the LFM project space (explore/nobackup/projects/lfm/vis_chips). \n",
    "\n",
    "## Input/label matching\n",
    "Labels and inputs were matched by asset ID, as well as tile row/column ID. \n",
    "\n",
    "## Training specifications\n",
    "Model was trained on 500 input/label pairs for 50 epochs, using a PRISM JupyterHub job on 4 V100 GPUs (1 V100 will also work, but will be slower). The parameters used were: \"combined\" loss function (Dice loss + Binary CE), 1e-4 LR, AdamW optimizer, and Cosine Annealing LR scheduling. A train/val split of 80/20% was used as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65889f6-4178-498f-8ffe-9aa405928235",
   "metadata": {},
   "source": [
    "## Imports, Dino Repo Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affb6f2-7114-45e7-af1a-0fefc17329a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from model import DINOSegmentation\n",
    "from dataset import get_dataloaders\n",
    "from driver import main\n",
    "\n",
    "# Notebook settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d85927-0978-48d0-b36d-72e2dc6a1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook requires access to models on GitHub (see their README)\n",
    "!git clone https://github.com/facebookresearch/dinov3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314109-cf55-417f-b553-7da23b0cdcba",
   "metadata": {},
   "source": [
    "## Main workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cd289-3975-4254-8d2c-faafee3babd4",
   "metadata": {},
   "source": [
    "### User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199a001-42fb-4a2d-996a-ae6d6ad6c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights URL (received after registering for DINOV3)\n",
    "weights_URL = (\n",
    "    \"https://dinov3.llamameta.net/dinov3_vitl16/\"\n",
    "    \"dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "    \"?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNDloYXZtdThkZGh3eGw3aH\"\n",
    "    \"JwNjQwa3E3IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXR\"\n",
    "    \"cLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE\"\n",
    "    \"3Njc5OTI2Njl9fX1dfQ__\"\n",
    "    \"&Signature=neHREO7xc90azhmnF3r9qPztYJ5L2wO-EZkVKh6ECzR5H2YGzdK3dcF\"\n",
    "    \"WQISNb6xYo3R5EO39FKJ7bwELXA%7EgoBqDbk-jm-9n9%7EVxtEOmWVx73usrILMwhyi\"\n",
    "    \"cP5-448rbnUzOEM0lPkGS829mOBJkaSxxSsbkQ0VpVBcScNEFcpaNOZ--BeHxCHdTFV\"\n",
    "    \"NGkhlEaCYPUbYyHYbTgDQntH2AsKYJTWw4NIEZJZLX9wjCOYKQ-YG86d0HJAvsdF79X\"\n",
    "    \"vITPgJSA0U-4Z1CzIkQhZb0N-7-XnbZmnJJi42xnNS0DsB6CTedxq0FAfiYklBY77wT\"\n",
    "    \"JrYLba%7Epkap23ymoUTxDXA__\"\n",
    "    \"&Key-Pair-Id=K15QRJLYKIFSLZ\"\n",
    "    \"&Download-Request-ID=1618342689192585\"\n",
    ")\n",
    "\n",
    "# Data paths\n",
    "INPUT_DIR = \"/explore/nobackup/people/ajkerr1/Lunar_FM\"\n",
    "IMAGE_DIR = f\"{INPUT_DIR}/vis_chips/chips\"\n",
    "LABEL_DIR = f\"{INPUT_DIR}/vis_chips/labels_npy\"\n",
    "\n",
    "# Output dir (create this if not already created)\n",
    "OUTPUT_DIR = \"\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Location of cloned dinov3 repo\n",
    "REPO_DIR = \"./dinov3\"\n",
    "\n",
    "# Dataset parameters\n",
    "MAX_SAMPLES = 500  # Set to None to use all available samples, or an integer to limit\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% validation\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "LOSS_TYPE = \"combined\"  # Combined Dice + Binary CE loss\n",
    "\n",
    "# Model parameters\n",
    "TARGET_SIZE = (304, 304)  # Input size for DINO model\n",
    "N_CLASSES = 2  # Binary segmentation (background, crater)\n",
    "FREEZE_ENCODER = True\n",
    "\n",
    "# Visualization and saving\n",
    "CHECKPOINT_EVERY = 10  # Save checkpoint every N epochs\n",
    "VISUALIZE_EVERY = 10  # Create visualizations every N epochs\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee23de-ee8d-4a92-a894-3afacb3b6667",
   "metadata": {},
   "source": [
    "### Training code\n",
    "1. Create dataloaders from files on /nobackup space.\n",
    "2. Load DinoV3 encoder, create encoder/decoder finetuning model.\n",
    "3. Train model.\n",
    "4. Print post-training QA stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f12b4-46d2-4fb9-a55f-967916c53b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Creating dataloaders.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_loader, val_loader, MEAN, STD = get_dataloaders(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_size=TARGET_SIZE,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    seed=42,\n",
    "    stats_save_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ENCODER AND CREATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Loading DINO encoder and creating model.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder = torch.hub.load(\n",
    "    REPO_DIR,\n",
    "    'dinov3_vitl16',\n",
    "    source='local',\n",
    "    weights=weights_URL\n",
    ").to(device)\n",
    "\n",
    "print(\"Encoder loaded with pretrained weights.\")\n",
    "\n",
    "# Create model with DINO segmentation head, UNet decoder (see model.py)\n",
    "print(\"Creating DINO segmentation model with UNet decoder...\")\n",
    "model = DINOSegmentation(\n",
    "    encoder=encoder,\n",
    "    num_classes=N_CLASSES,\n",
    "    img_size=TARGET_SIZE\n",
    ").to(device)\n",
    "\n",
    "# Unfreeze encoder for full fine-tuning\n",
    "if FREEZE_ENCODER:\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Encoder frozen (only decoder will be trained).\")\n",
    "else:\n",
    "    print(\"Encoder unfrozen! Full model will be trained.\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    mode=\"train\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    checkpoint_every=CHECKPOINT_EVERY,\n",
    "    visualize_every=VISUALIZE_EVERY,\n",
    "    loss_type=LOSS_TYPE,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV Kernel",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
