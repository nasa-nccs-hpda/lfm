{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b872aa-c1e9-42b8-b966-2ca81a5c9315",
   "metadata": {},
   "source": [
    "# LFM Segmentation Example Workflow\n",
    "This notebook is an example workflow of doing binary segmentation on visual light (RGB) bands of Lunar data. \n",
    "\n",
    "You can get started with this notebook by downloading it with:\n",
    "\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/nasa-nccs-hpda/lfm/refs/heads/main/notebooks/finetune_dinov3.ipynb\n",
    "```\n",
    "\n",
    "**See the README in the [repo](https://github.com/nasa-nccs-hpda/lfm)** for more info on how to use this notebook, and more on the process of training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65889f6-4178-498f-8ffe-9aa405928235",
   "metadata": {},
   "source": [
    "## Imports, Dino Repo Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affb6f2-7114-45e7-af1a-0fefc17329a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738befb-d936-459f-9163-7928396eeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"lfm\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.run([\"git\", \"clone\", f\"https://github.com/nasa-nccs-hpda/{repo_dir}\"])\n",
    "else:\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09248022-9c48-4396-a4c1-135e8abfbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"lfm\")\n",
    "\n",
    "from lfm.tasks.segmentation.model import DINOSegmentation, load_dinov3_encoder\n",
    "from lfm.tasks.segmentation.dataset import get_dataloaders\n",
    "from lfm.tasks.segmentation.driver import train_model\n",
    "from lfm.tasks.segmentation.utils import install_termcolor_locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c898a-13bd-4d48-9c8a-341bc027b090",
   "metadata": {},
   "source": [
    "### Install termcolor package (required by DinoV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3634-069f-4c16-89f0-09eb11e071d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_termcolor_locally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314109-cf55-417f-b553-7da23b0cdcba",
   "metadata": {},
   "source": [
    "## Main workflow\n",
    "\n",
    "1. Define user-configured variables\n",
    "2. Create dataloaders from files on /explore/nobackup/.\n",
    "3. Load DinoV3 encoder, create encoder/decoder finetuning model.\n",
    "4. Train model, print training stats, and visualize results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cd289-3975-4254-8d2c-faafee3babd4",
   "metadata": {},
   "source": [
    "### User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199a001-42fb-4a2d-996a-ae6d6ad6c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights URL (received after registering for DINOV3)\n",
    "weights_URL = (\n",
    "    \"https://dinov3.llamameta.net/dinov3_vitl16/\"\n",
    "    \"dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "    \"?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNDloYXZtdThkZGh3eGw3aH\"\n",
    "    \"JwNjQwa3E3IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXR\"\n",
    "    \"cLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE\"\n",
    "    \"3Njc5OTI2Njl9fX1dfQ__\"\n",
    "    \"&Signature=neHREO7xc90azhmnF3r9qPztYJ5L2wO-EZkVKh6ECzR5H2YGzdK3dcF\"\n",
    "    \"WQISNb6xYo3R5EO39FKJ7bwELXA%7EgoBqDbk-jm-9n9%7EVxtEOmWVx73usrILMwhyi\"\n",
    "    \"cP5-448rbnUzOEM0lPkGS829mOBJkaSxxSsbkQ0VpVBcScNEFcpaNOZ--BeHxCHdTFV\"\n",
    "    \"NGkhlEaCYPUbYyHYbTgDQntH2AsKYJTWw4NIEZJZLX9wjCOYKQ-YG86d0HJAvsdF79X\"\n",
    "    \"vITPgJSA0U-4Z1CzIkQhZb0N-7-XnbZmnJJi42xnNS0DsB6CTedxq0FAfiYklBY77wT\"\n",
    "    \"JrYLba%7Epkap23ymoUTxDXA__\"\n",
    "    \"&Key-Pair-Id=K15QRJLYKIFSLZ\"\n",
    "    \"&Download-Request-ID=1618342689192585\"\n",
    ")\n",
    "weights_local_checkpoint = '/explore/nobackup/projects/ilab/models/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth'\n",
    "\n",
    "# Data paths\n",
    "INPUT_DIR = \"/explore/nobackup/projects/lfm\"\n",
    "IMAGE_DIR = f\"{INPUT_DIR}/vis_chips/chips\"\n",
    "LABEL_DIR = f\"{INPUT_DIR}/vis_chips/labels_npy\"\n",
    "\n",
    "# Output dir (this will be created automatically)\n",
    "OUTPUT_DIR = \"./outputs\"  # Change this if you want a specific path\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Location of cloned dinov3 repo\n",
    "REPO_DIR = \"./dinov3\"\n",
    "\n",
    "# Dataset parameters\n",
    "MAX_SAMPLES = 500  # Set to None to use all available samples, or an integer to limit\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% validation\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "LOSS_TYPE = \"combined\"  # Combined Dice + Binary CE loss\n",
    "\n",
    "# Model parameters\n",
    "TARGET_SIZE = (304, 304)  # Input size for DINO model\n",
    "FREEZE_ENCODER = False\n",
    "\n",
    "# Visualization and saving\n",
    "CHECKPOINT_EVERY = 10  # Save checkpoint every N epochs\n",
    "VISUALIZE_EVERY = 10  # Create visualizations every N epochs\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee23de-ee8d-4a92-a894-3afacb3b6667",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f12b4-46d2-4fb9-a55f-967916c53b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Creating dataloaders.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_loader, val_loader, MEAN, STD = get_dataloaders(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_size=TARGET_SIZE,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    seed=42,\n",
    "    stats_save_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0977cf-840d-4b68-b26a-f9e84a9a1685",
   "metadata": {},
   "source": [
    "### Load Encoder and Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac112f0b-46c0-4c61-a17c-3063c3eae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Loading DINO encoder and creating model.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder = load_dinov3_encoder(weights_local_checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f359b-922e-42e2-8e4f-f84dfb927016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with DINO segmentation head, UNet decoder (see model.py)\n",
    "print(\"Creating DINO segmentation model with UNet decoder...\")\n",
    "model = DINOSegmentation(\n",
    "    encoder=encoder,  # Use DINOv3 head\n",
    "    num_classes=2,  # Binary segmentaton (crater, not crater)\n",
    "    img_size=TARGET_SIZE\n",
    ").to(device)\n",
    "\n",
    "# Unfreeze encoder for full fine-tuning if desired\n",
    "if FREEZE_ENCODER:\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Encoder frozen (only decoder will be trained).\")\n",
    "else:\n",
    "    print(\"Encoder unfrozen! Full model will be trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57daa71-8813-4dbd-8dd1-f899614605a9",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380a15c-cef9-449e-ae2b-9758b58545f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    mode=\"train\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    checkpoint_every=CHECKPOINT_EVERY,\n",
    "    visualize_every=VISUALIZE_EVERY,\n",
    "    loss_type=LOSS_TYPE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c408068-1cae-408f-af54-ce0d541f31af",
   "metadata": {},
   "source": [
    "## Display some of the output visualizations\n",
    "\n",
    "The training of the model is already producing some visualizations every N epochs.\n",
    "Here we open some of the visualizations to look at them from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c449d-8855-4866-ae9f-38463bc320bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_dir = os.path.join(OUTPUT_DIR, 'visualizations')\n",
    "visualization_filenames = sorted(glob(os.path.join(visualization_dir, '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35e283-1346-4ec6-8696-25eadd422af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vis_filename in visualization_filenames:\n",
    "\n",
    "    img = mpimg.imread(vis_filename)\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV Kernel",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
