{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b872aa-c1e9-42b8-b966-2ca81a5c9315",
   "metadata": {},
   "source": [
    "# LFM Segmentation Example Workflow\n",
    "This notebook is an example workflow of doing binary segmentation on visual light (RGB) bands of Lunar data. \n",
    "\n",
    "**See the README in the [repo](https://github.com/nasa-nccs-hpda/lfm)** for more info on how to use this notebook, and more on the process of training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65889f6-4178-498f-8ffe-9aa405928235",
   "metadata": {},
   "source": [
    "## Imports, Dino Repo Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5affb6f2-7114-45e7-af1a-0fefc17329a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from lfm.tasks.segmentation.model import DINOSegmentation\n",
    "from lfm.tasks.segmentation.dataset import get_dataloaders\n",
    "from lfm.tasks.segmentation.driver import train_model\n",
    "from lfm.tasks.segmentation.utils import install_termcolor_locally\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c898a-13bd-4d48-9c8a-341bc027b090",
   "metadata": {},
   "source": [
    "### Install termcolor package (required by DinoV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ab3634-069f-4c16-89f0-09eb11e071d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor\n",
      "  Using cached termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Using cached termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-3.3.0\n",
      "Termcolor installed to: /home/ajkerr1/.local/lib/python3.12/site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Target directory /home/ajkerr1/.local/lib/python3.12/site-packages/termcolor already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/ajkerr1/.local/lib/python3.12/site-packages/termcolor-3.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "install_termcolor_locally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b181dc-8375-40a4-9396-1e1353b46314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/dinov3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314109-cf55-417f-b553-7da23b0cdcba",
   "metadata": {},
   "source": [
    "## Main workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cd289-3975-4254-8d2c-faafee3babd4",
   "metadata": {},
   "source": [
    "### User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9199a001-42fb-4a2d-996a-ae6d6ad6c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./outputs\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Weights URL (received after registering for DINOV3)\n",
    "weights_URL = (\n",
    "    \"https://dinov3.llamameta.net/dinov3_vitl16/\"\n",
    "    \"dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "    \"?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNDloYXZtdThkZGh3eGw3aH\"\n",
    "    \"JwNjQwa3E3IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXR\"\n",
    "    \"cLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE\"\n",
    "    \"3Njc5OTI2Njl9fX1dfQ__\"\n",
    "    \"&Signature=neHREO7xc90azhmnF3r9qPztYJ5L2wO-EZkVKh6ECzR5H2YGzdK3dcF\"\n",
    "    \"WQISNb6xYo3R5EO39FKJ7bwELXA%7EgoBqDbk-jm-9n9%7EVxtEOmWVx73usrILMwhyi\"\n",
    "    \"cP5-448rbnUzOEM0lPkGS829mOBJkaSxxSsbkQ0VpVBcScNEFcpaNOZ--BeHxCHdTFV\"\n",
    "    \"NGkhlEaCYPUbYyHYbTgDQntH2AsKYJTWw4NIEZJZLX9wjCOYKQ-YG86d0HJAvsdF79X\"\n",
    "    \"vITPgJSA0U-4Z1CzIkQhZb0N-7-XnbZmnJJi42xnNS0DsB6CTedxq0FAfiYklBY77wT\"\n",
    "    \"JrYLba%7Epkap23ymoUTxDXA__\"\n",
    "    \"&Key-Pair-Id=K15QRJLYKIFSLZ\"\n",
    "    \"&Download-Request-ID=1618342689192585\"\n",
    ")\n",
    "\n",
    "# Data paths\n",
    "INPUT_DIR = \"/explore/nobackup/projects/lfm\" \n",
    "IMAGE_DIR = f\"{INPUT_DIR}/vis_chips/chips\"\n",
    "LABEL_DIR = f\"{INPUT_DIR}/vis_chips/labels_npy\"\n",
    "\n",
    "# Output dir (this will be created automatically)\n",
    "OUTPUT_DIR = \"./outputs\"  # Change this if you want a specific path\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Location of cloned dinov3 repo\n",
    "REPO_DIR = \"./dinov3\"\n",
    "\n",
    "# Dataset parameters\n",
    "MAX_SAMPLES = 500  # Set to None to use all available samples, or an integer to limit\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% validation\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "LOSS_TYPE = \"combined\"  # Combined Dice + Binary CE loss\n",
    "\n",
    "# Model parameters\n",
    "TARGET_SIZE = (304, 304)  # Input size for DINO model\n",
    "FREEZE_ENCODER = True\n",
    "\n",
    "# Visualization and saving\n",
    "CHECKPOINT_EVERY = 10  # Save checkpoint every N epochs\n",
    "VISUALIZE_EVERY = 10  # Create visualizations every N epochs\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee23de-ee8d-4a92-a894-3afacb3b6667",
   "metadata": {},
   "source": [
    "### Training code\n",
    "1. Create dataloaders from files on /nobackup space.\n",
    "2. Load DinoV3 encoder, create encoder/decoder finetuning model.\n",
    "3. Train model, print training stats, and visualize results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f12b4-46d2-4fb9-a55f-967916c53b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Creating dataloaders.\n",
      "============================================================\n",
      "Computing dataset statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing dataset statistics:   0%|          | 0/7530 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image shape: (3, 300, 300), dtype: float64\n",
      "Value range: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing dataset statistics:  17%|█▋        | 1286/7530 [00:11<00:55, 112.95it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Creating dataloaders.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_loader, val_loader, MEAN, STD = get_dataloaders(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_size=TARGET_SIZE,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    seed=42,\n",
    "    stats_save_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ENCODER AND CREATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Loading DINO encoder and creating model.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encoder = torch.hub.load(\n",
    "    repo_or_dir='facebookresearch/dinov3',  # GitHub repo\n",
    "    model='dinov3_vitl16',\n",
    "    source='github',\n",
    "    weights=weights_URL\n",
    ").to(device)\n",
    "\n",
    "print(\"Encoder loaded with pretrained weights.\")\n",
    "\n",
    "# Create model with DINO segmentation head, UNet decoder (see model.py)\n",
    "print(\"Creating DINO segmentation model with UNet decoder...\")\n",
    "model = DINOSegmentation(\n",
    "    encoder=encoder,  # Use DINOv3 head\n",
    "    num_classes=2,  # Binary segmentaton (crater, not crater)\n",
    "    img_size=TARGET_SIZE\n",
    ").to(device)\n",
    "\n",
    "# Unfreeze encoder for full fine-tuning if desired\n",
    "if FREEZE_ENCODER:\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Encoder frozen (only decoder will be trained).\")\n",
    "else:\n",
    "    print(\"Encoder unfrozen! Full model will be trained.\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    mode=\"train\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    checkpoint_every=CHECKPOINT_EVERY,\n",
    "    visualize_every=VISUALIZE_EVERY,\n",
    "    loss_type=LOSS_TYPE,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV Kernel",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
